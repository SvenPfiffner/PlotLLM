import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import io

from chat import assistant_response, check_service

# Set up Streamlit page
st.set_page_config(page_title="PlotLLM: AI Plot Generator", layout="wide")

# Disclaimer
@st.dialog("Disclaimer")
def disclaimer():
    st.write("This demo uses a locally running deepseek-r1 reasoning model to generate code based on your prompts. Please note that this software is a proof of concept and should not be used in production as it contains several security vulnerabilities. For example, code generated by deepseek is automatically executed, which could lead to arbitrary code execution when exploiting user prompts. By using this demo, you agree that you are aware of this and proceed at your own risk.")
    st.write("Deepseek-r1 is queried via ollama. Please make sure that the service is running on your machine before using this demo.")
    if st.button("I understand"):
        st.session_state.accept_disc = True
        st.rerun()

if "accept_disc" not in st.session_state:
    disclaimer()

st.title("📊 PlotLM: AI Plot Generator")
st.markdown("Made with ❤️ by [Sven Pfiffner](https://github.com/SvenPfiffner/)")

# Settings
with st.sidebar.expander("⚙️ Settings"):
    reset_btn = st.button("🗑️ Reset Chat History")
    model_select = st.selectbox("Select Model", ["deepseek-r1:1.5b",
                                                 "deepseek-r1:7b",
                                                 "deepseek-r1:8b",
                                                 "deepseek-r1:14b",
                                                 "deepseek-r1:32b",
                                                 "deepseek-r1:70b",
                                                 "deepseek-r1:671b",])

    model_requirements = { "deepseek-r1:1.5b": "1.1GB",
                            "deepseek-r1:7b": "4.7GB",
                            "deepseek-r1:8b": "4.9GB",
                            "deepseek-r1:14b": "9.0GB",
                            "deepseek-r1:32b": "20GB",
                            "deepseek-r1:70b": "43GB",
                            "deepseek-r1:671b": "404GB"}

    st.write(f"⚠️ Model size requirement: {model_requirements[model_select]}")

if reset_btn:
    st.session_state["messages"] = []
    st.session_state["response_code"] = ""

# Sidebar for file upload
st.sidebar.header("Output")

# Initialize session state for chat history
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# Initialize session state for code
if "response_code" not in st.session_state:
    st.session_state["response_code"] = ""

# User query input
user_input = st.chat_input("Describe the plot you want")

if user_input:

    # Check if ollama service is running
    if not check_service():
        st.toast("Ollama service could not be reached. Please make sure Ollama is running on your machine and try again", icon="🚨")
    else:
        st.session_state["messages"].append({"role": "user", "content": user_input})

        #if uploaded_file is not None:
        #df = pd.read_csv(uploaded_file)
        #st.session_state["dataframe"] = "(0,0), (1,1), (2,2), (3,3)"

        # Use AI to generate Matplotlib code (example using OpenAI API)
        response_text, response_code = assistant_response(user_input, st.session_state["messages"], model=model_select)

        st.session_state["messages"].append({"role": "assistant", "content": response_text})
        st.session_state["response_code"] = response_code
        #else:
            #st.error("Please upload a CSV file first.")

    # Display chat history
    for message in st.session_state["messages"]:
        st.chat_message(message["role"]).write(message["content"])

# Display generated code
st.sidebar.subheader("📝 Generated Python Code")
if "response_code" in st.session_state and len(st.session_state["response_code"]) >= 1:
    code = st.session_state["response_code"]
    st.sidebar.code(code, language="python")
    st.sidebar.button("Copy Code", on_click=lambda: st.session_state.update({"copy_code": code}))

# Execute generated code and show plot
st.sidebar.subheader("📊 Generated Plot")
if "response_code" in st.session_state and len(st.session_state["response_code"]) >= 1:
    code_to_run = st.session_state["response_code"]
    try:
        exec(code_to_run, locals())
        st.sidebar.pyplot()

    except Exception as e:
        st.error(f"Error executing code: {e}")
